{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bc2919-f1e4-426f-9054-26e621901612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "237df5c3-c6af-401e-9a9e-9b4e3876c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from Processing notebook\n",
    "\n",
    "data = np.load(\"combined_representations.npz\", allow_pickle=True)\n",
    "X_train_combined = data[\"X_train\"]\n",
    "X_val_combined = data[\"X_val\"]\n",
    "test_combined = data[\"test\"]\n",
    "y_train = data[\"y_train\"]\n",
    "y_val = data[\"y_val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5686cd31-0d17-4786-9d2f-b837de19c505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'description': array([[ 0.        ,  0.        ,  0.        , ..., -0.26894767,\n",
       "         0.28410659, -0.05457572],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.18193519,\n",
       "         0.56684933,  0.1033013 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.15412894,\n",
       "         0.41844097, -0.1535055 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.20144454,\n",
       "         0.56314219, -0.01820332],\n",
       "       [ 0.        ,  0.        ,  0.11503044, ..., -0.09101021,\n",
       "         0.50645848,  0.04244453],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.29217856,\n",
       "         0.44455622,  0.144129  ]]), 'host_about': array([[ 0.        ,  0.        ,  0.        , ..., -0.12193654,\n",
       "        -0.0445922 , -0.00576038],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.1794887 ,\n",
       "         0.44336581,  0.22292026],\n",
       "       [ 0.        ,  0.08749985,  0.08937212, ..., -0.03605678,\n",
       "         0.48175168,  0.20329235],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.21277664,\n",
       "         0.22909964,  0.1939016 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.16944592,\n",
       "         0.31317102,  0.0449433 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.25649588,\n",
       "         0.346195  ,  0.22571714]]), 'comments': array([[ 0.0050339 ,  0.        ,  0.0057177 , ..., -0.15276542,\n",
       "         0.26823887,  0.10006609],\n",
       "       [ 0.0603424 ,  0.        ,  0.        , ..., -0.1225307 ,\n",
       "         0.28384773,  0.01153072],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.00670126,  0.        ,  0.01522313, ..., -0.36147214,\n",
       "         0.13623308,  0.1308486 ],\n",
       "       [ 0.14658567,  0.04840237,  0.        , ..., -0.04748795,\n",
       "         0.38492061,  0.08104253]])}, dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c57e753-24e6-4c7f-9c71-7e3fb930126a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'description': array([[ 0.        ,  0.        ,  0.        , ..., -0.28789246,\n",
       "         0.48497257,  0.15152374],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.06563767,\n",
       "         0.40370935,  0.0725085 ],\n",
       "       [ 0.        ,  0.        ,  0.13880652, ..., -0.16953171,\n",
       "         0.3015066 ,  0.14860522],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.10403167,\n",
       "        -0.09614274,  0.04147864],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.16573102,\n",
       "         0.38612467,  0.25377321],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.121358  ,\n",
       "         0.53756869,  0.1067788 ]]), 'host_about': array([[ 0.        ,  0.        ,  0.        , ..., -0.36170898,\n",
       "         0.30741972,  0.08949168],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.11652934,\n",
       "         0.34135978,  0.214483  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.20559426,\n",
       "         0.22731568,  0.1786795 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.00849472,\n",
       "         0.0098264 , -0.0045921 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.13237719,\n",
       "         0.15893128,  0.11644312],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.03293796,\n",
       "         0.16548174,  0.02213182]]), 'comments': array([[ 0.00494471,  0.        ,  0.01697014, ..., -0.33282122,\n",
       "         0.1057969 ,  0.14543912],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.01266679,  0.        , ..., -0.2065221 ,\n",
       "         0.31429807,  0.26120829],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.22302706,\n",
       "         0.21273879,  0.25610776],\n",
       "       [ 0.02607374,  0.        ,  0.00835189, ..., -0.09930663,\n",
       "         0.30802296,  0.1959286 ]])}, dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4637502c-e231-4774-8993-fd4c8191e286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'description': array([[ 0.        ,  0.        ,  0.        , ..., -0.18555058,\n",
       "         0.44161261, -0.0627408 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.0345583 ,\n",
       "         0.04763634, -0.02800948],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.08426904,\n",
       "         0.34936657,  0.0997809 ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.21587891,\n",
       "         0.39432905,  0.11549282],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.23952458,\n",
       "         0.445471  ,  0.19499586],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.12781864,\n",
       "         0.43982258,  0.14028625]]), 'host_about': array([[ 0.        ,  0.        ,  0.        , ..., -0.28434646,\n",
       "        -0.07843604, -0.0471223 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.0291326 ,\n",
       "         0.04489934, -0.0054476 ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.25649588,\n",
       "         0.346195  ,  0.22571714],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.13211228,\n",
       "         0.29305133,  0.14286702],\n",
       "       [ 0.11943491,  0.        ,  0.        , ..., -0.03605678,\n",
       "         0.48175168,  0.20329235],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.22638684,\n",
       "         0.37341186,  0.21335005]]), 'comments': array([[ 0.03444486,  0.        ,  0.03870422, ..., -0.3089906 ,\n",
       "         0.55383694,  0.17026391],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.1933527 ,\n",
       "         0.50444908,  0.2194604 ],\n",
       "       ...,\n",
       "       [ 0.00931739,  0.01294997,  0.0209391 , ..., -0.21715889,\n",
       "         0.25644795,  0.17570238],\n",
       "       [ 0.02740491,  0.        ,  0.        , ..., -0.09983712,\n",
       "         0.27176506,  0.114914  ],\n",
       "       [ 0.        ,  0.        ,  0.        , ..., -0.11258514,\n",
       "         0.415003  ,  0.17261918]])}, dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8c433e-8492-43b8-85f2-44b34b39519b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1562,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a07d76-e702-454b-b643-1560e7109963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4686,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512ba27-bdf7-4f69-b05b-f08380e4b533",
   "metadata": {},
   "source": [
    "### Preparing data for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f9eca1-d8b2-4b2d-80c5-c99b2e62e658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged feature matrix: (4686, 1800)\n"
     ]
    }
   ],
   "source": [
    "# For X_train_combined\n",
    "\n",
    "train_dict = X_train_combined.item()\n",
    "\n",
    "description_features_train = train_dict['description']\n",
    "host_about_features_train = train_dict['host_about']\n",
    "comments_features_train = train_dict['comments']\n",
    "\n",
    "X_train_merged = np.concatenate((description_features_train, host_about_features_train, comments_features_train), axis=1)\n",
    "print(\"Shape of merged feature matrix:\", X_train_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c769012-d604-48d1-b852-1edec788bdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged feature matrix: (1562, 1800)\n"
     ]
    }
   ],
   "source": [
    "# For X_val_combined\n",
    "\n",
    "val_dict = X_val_combined.item()\n",
    "\n",
    "description_features_val = val_dict['description']\n",
    "host_about_features_val = val_dict['host_about']\n",
    "comments_features_val = val_dict['comments']\n",
    "\n",
    "X_val_merged = np.concatenate((description_features_val, host_about_features_val, comments_features_val), axis=1)\n",
    "print(\"Shape of merged feature matrix:\", X_val_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56bfb901-e737-4717-9883-9c742c0c1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged feature matrix: (695, 1800)\n"
     ]
    }
   ],
   "source": [
    "# For test_combined\n",
    "\n",
    "test_dict = test_combined.item()\n",
    "\n",
    "description_features_test = test_dict['description']\n",
    "host_about_features_test = test_dict['host_about']\n",
    "comments_features_test = test_dict['comments']\n",
    "\n",
    "X_test_merged = np.concatenate((description_features_test, host_about_features_test, comments_features_test), axis=1)\n",
    "print(\"Shape of merged feature matrix:\", X_test_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb14b7-15ab-434d-9cf2-be878fa46060",
   "metadata": {},
   "source": [
    "# Models\n",
    "##### Grid Searches are small because that's not the main point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf433c9-1de5-4acb-8bb0-c42da7714461",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f39e3d2-4ea9-40ec-bade-29d7cdd22a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:47<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 0.05, 'penalty': 'l2', 'solver': 'saga'}\n",
      "F1 score on validation set with best parameters: 0.8597951344430218\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.86      0.90      1135\n",
      "           1       0.70      0.85      0.77       427\n",
      "\n",
      "    accuracy                           0.86      1562\n",
      "   macro avg       0.82      0.86      0.83      1562\n",
      "weighted avg       0.87      0.86      0.86      1562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[980 155]\n",
      " [ 64 363]]\n",
      "F1 score on validation set: 0.8636181019158093\n"
     ]
    }
   ],
   "source": [
    "# Define values for grid search\n",
    "parameters = {\n",
    "    'C': [0.005, 0.01, 0.05],  \n",
    "    'penalty': ['l1', 'l2'],  \n",
    "    'solver': ['liblinear', 'saga'],      \n",
    "}\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "# Progress bar\n",
    "total_combinations = len(parameters['C']) * len(parameters['penalty']) * len(parameters['solver'])\n",
    "pbar = tqdm(total=total_combinations)\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for C, penalty, solver in product(parameters['C'], parameters['penalty'], parameters['solver']):\n",
    "\n",
    "    pbar.update(1)\n",
    "    lr = LogisticRegression(C=C, penalty=penalty, solver=solver, random_state=0)\n",
    "\n",
    "    lr.fit(X_train_merged, y_train)\n",
    "    f1 = f1_score(y_val, lr.predict(X_val_merged), average='weighted')\n",
    "    \n",
    "    # Check if score is the best\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = {'C': C, 'penalty': penalty, 'solver': solver}\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Use the best parameter combination found\n",
    "print(\"Best parameters found:\", best_params)\n",
    "best_lr = LogisticRegression(**best_params, random_state=0)\n",
    "best_lr.fit(X_train_merged, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_pred = best_lr.predict(X_val_merged)\n",
    "\n",
    "print(\"F1 score on validation set with best parameters:\", best_lr.score(X_val_merged, y_val))\n",
    "print(classification_report(y_val, lr_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, lr_pred))\n",
    "\n",
    "f1 = f1_score(y_val, lr_pred, average='weighted')\n",
    "print(\"F1 score on validation set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0f1f3bb-0fac-44fc-be60-dc9bd9e0752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1: 0.8636181019158093 ; for: {'C': 0.05, 'penalty': 'l2', 'solver': 'saga'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd226f-9b73-41bd-a448-1a0963513e5c",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8347d49-0637-497b-ba3c-5abba3612964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [09:05<00:00, 22.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto'}\n",
      "F1 score on validation set with best parameters: 0.8540332906530089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      1135\n",
      "           1       0.71      0.78      0.74       427\n",
      "\n",
      "    accuracy                           0.85      1562\n",
      "   macro avg       0.81      0.83      0.82      1562\n",
      "weighted avg       0.86      0.85      0.86      1562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1003  132]\n",
      " [  96  331]]\n",
      "F1 score on validation set: 0.8558093293162234\n"
     ]
    }
   ],
   "source": [
    "# Values for grid search\n",
    "parameters = {\n",
    "    'n_neighbors': [3, 5, 7],    \n",
    "    'weights': ['uniform', 'distance'],    \n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],   \n",
    "}\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "# Progress bar\n",
    "total_combinations = len(parameters['n_neighbors']) * len(parameters['weights']) * len(parameters['algorithm'])\n",
    "pbar = tqdm(total=total_combinations)\n",
    "\n",
    "# Iterate over all combinations of parameters\n",
    "for n_neighbors, weights, algorithm in product(parameters['n_neighbors'], parameters['weights'], parameters['algorithm']):\n",
    "\n",
    "    pbar.update(1)\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm)\n",
    "    \n",
    "    knn.fit(X_train_merged, y_train)\n",
    "    knn_pred = knn.predict(X_val_merged)\n",
    "    \n",
    "    f1 = f1_score(y_val, knn_pred, average='weighted')\n",
    "    \n",
    "    # Check if score is the best\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = {'n_neighbors': n_neighbors, 'weights': weights, 'algorithm': algorithm}\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Use the best model found\n",
    "print(\"Best parameters found:\", best_params)\n",
    "best_knn = KNeighborsClassifier(**best_params)\n",
    "best_knn.fit(X_train_merged, y_train)\n",
    "\n",
    "# Predictions\n",
    "knn_pred = best_knn.predict(X_val_merged)\n",
    "\n",
    "print(\"F1 score on validation set with best parameters:\", best_knn.score(X_val_merged, y_val))\n",
    "print(classification_report(y_val, knn_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, knn_pred))\n",
    "\n",
    "f1 = f1_score(y_val, knn_pred, average='weighted')\n",
    "print(\"F1 score on validation set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72cc6482-7fd0-4591-a4dc-5569467a9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1: 0.8558093293162234 ; for: {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560e72a-cbdf-4a1f-bf93-4deb3cc4e538",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee9646f9-9885-4e72-82a1-5e3da5172c44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5/18 [00:20<00:59,  4.57s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 33%|███▎      | 6/18 [00:26<01:01,  5.15s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 39%|███▉      | 7/18 [00:50<02:03, 11.23s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 50%|█████     | 9/18 [01:01<01:11,  7.93s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 56%|█████▌    | 10/18 [01:08<01:02,  7.82s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 11/18 [01:36<01:36, 13.78s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 67%|██████▋   | 12/18 [01:44<01:12, 12.12s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 72%|███████▏  | 13/18 [02:12<01:24, 16.86s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 83%|████████▎ | 15/18 [02:24<00:33, 11.30s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      " 89%|████████▉ | 16/18 [02:33<00:21, 10.72s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      " 94%|█████████▍| 17/18 [03:03<00:16, 16.37s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:541: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "100%|██████████| 18/18 [03:11<00:00, 13.84s/it]C:\\Users\\jaime\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "100%|██████████| 18/18 [03:37<00:00, 12.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'hidden_layer_sizes': (2, 2), 'activation': 'relu', 'solver': 'lbfgs'}\n",
      "F1 score on validation set with best parameters: 0.8706786171574904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      1135\n",
      "           1       0.73      0.85      0.78       427\n",
      "\n",
      "    accuracy                           0.87      1562\n",
      "   macro avg       0.83      0.86      0.84      1562\n",
      "weighted avg       0.88      0.87      0.87      1562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[999 136]\n",
      " [ 66 361]]\n",
      "F1 score on validation set: 0.8735197687502425\n"
     ]
    }
   ],
   "source": [
    "# Parameters for grid search\n",
    "parameters = {\n",
    "    'hidden_layer_sizes': [(2, 2), (5, 5), (2, 2, 2)],    \n",
    "    'activation': ['logistic', 'relu', 'tanh'],    \n",
    "    'solver': ['lbfgs', 'sgd'],     \n",
    "}\n",
    "\n",
    "best_f1 = 0\n",
    "best_params = None\n",
    "\n",
    "# Progress bar\n",
    "total_combinations = len(parameters['hidden_layer_sizes']) * len(parameters['activation']) * len(parameters['solver'])\n",
    "pbar = tqdm(total=total_combinations)\n",
    "\n",
    "# Perform grid search\n",
    "for hidden_layer_sizes, activation, solver in product(parameters['hidden_layer_sizes'], parameters['activation'], parameters['solver']):\n",
    "\n",
    "    pbar.update(1)\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, random_state=0)\n",
    "\n",
    "    mlp.fit(X_train_merged, y_train)\n",
    "    mlp_pred = mlp.predict(X_val_merged)\n",
    "\n",
    "    f1 = f1_score(y_val, mlp_pred, average='weighted')\n",
    "    \n",
    "    # Check if best score\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = {'hidden_layer_sizes': hidden_layer_sizes, 'activation': activation, 'solver': solver}\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Use the best model found by manual search\n",
    "print(\"Best parameters found:\", best_params)\n",
    "best_mlp = MLPClassifier(**best_params, random_state=0)\n",
    "best_mlp.fit(X_train_merged, y_train)\n",
    "\n",
    "# Predictions\n",
    "mlp_pred = best_mlp.predict(X_val_merged)\n",
    "\n",
    "print(\"F1 score on validation set with best parameters:\", best_mlp.score(X_val_merged, y_val))\n",
    "print(classification_report(y_val, mlp_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, mlp_pred))\n",
    "\n",
    "f1 = f1_score(y_val, mlp_pred, average='weighted')\n",
    "print(\"F1 score on validation set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c782299-010b-4465-999f-bdb23ac75a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1: 0.8735197687502425 ; for: {'hidden_layer_sizes': (2, 2), 'activation': 'relu', 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f4ee8e-9e4b-4185-9e83-9d26b19514e1",
   "metadata": {},
   "source": [
    "# Extra Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b7adf-ce8f-4741-ac33-fc162a705dc0",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8a41ba5-54e2-46ae-a80d-bd9f4f4bb82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [23:27<00:00, 39.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found for Random Forest: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "F1 score on validation set with best parameters for Random Forest: 0.8847631241997439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      1135\n",
      "           1       0.77      0.83      0.80       427\n",
      "\n",
      "    accuracy                           0.88      1562\n",
      "   macro avg       0.85      0.87      0.86      1562\n",
      "weighted avg       0.89      0.88      0.89      1562\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "[[1026  109]\n",
      " [  71  356]]\n",
      "F1 score on validation set for Random Forest: 0.8862367622618262\n"
     ]
    }
   ],
   "source": [
    "# Values for grid search\n",
    "parameters_rf = {\n",
    "    'n_estimators': [100, 200],     \n",
    "    'max_depth': [None, 10, 20],      \n",
    "    'min_samples_split': [2, 5, 10],      \n",
    "    'min_samples_leaf': [1, 2]     \n",
    "}\n",
    "\n",
    "best_f1_rf = 0\n",
    "best_params_rf = None\n",
    "\n",
    "# Progression bar\n",
    "total_combinations_rf = len(parameters_rf['n_estimators']) * len(parameters_rf['max_depth']) * len(parameters_rf['min_samples_split']) * len(parameters_rf['min_samples_leaf'])\n",
    "pbar_rf = tqdm(total=total_combinations_rf)\n",
    "\n",
    "# Perform grid search\n",
    "for n_estimators, max_depth, min_samples_split, min_samples_leaf in product(parameters_rf['n_estimators'], parameters_rf['max_depth'], parameters_rf['min_samples_split'], parameters_rf['min_samples_leaf']):\n",
    "\n",
    "    pbar_rf.update(1)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
    "    rf.fit(X_train_merged, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_pred = rf.predict(X_val_merged)\n",
    "    f1_rf = f1_score(y_val, rf_pred, average='weighted')\n",
    "    \n",
    "    # Check if it's the best score\n",
    "    if f1_rf > best_f1_rf:\n",
    "        best_f1_rf = f1_rf\n",
    "        best_params_rf = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "pbar_rf.close()\n",
    "\n",
    "# Use the best model found\n",
    "print(\"Best parameters found for Random Forest:\", best_params_rf)\n",
    "best_rf = RandomForestClassifier(**best_params_rf, random_state=0)\n",
    "best_rf.fit(X_train_merged, y_train)\n",
    "\n",
    "# Predictions\n",
    "rf_pred = best_rf.predict(X_val_merged)\n",
    "\n",
    "print(\"F1 score on validation set with best parameters for Random Forest:\", best_rf.score(X_val_merged, y_val))\n",
    "print(classification_report(y_val, rf_pred))\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_val, rf_pred))\n",
    "\n",
    "f1_rf = f1_score(y_val, rf_pred, average='weighted')\n",
    "print(\"F1 score on validation set for Random Forest:\", f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1d301c7-29ea-41ae-a1da-a2afcb7f54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1: 0.8862367622618262 ; for: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07639de-f038-4001-8ed7-e1f2f082c550",
   "metadata": {},
   "source": [
    "### Transformer based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1327f3fb-09b6-4dc2-b5ac-921c0bd2fd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install transformers[torch] accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9dc2cb-543b-4972-83ea-fe6717fc63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.load(\"transformers_embedding.npz\")\n",
    "\n",
    "X_train_tf = data['X_train']\n",
    "X_val_tf = data['X_val']\n",
    "test_tf = data['test']\n",
    "y_train_tf = data['y_train']\n",
    "y_val_tf = data['y_val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30cd3f2-460f-4a3b-8574-c476a12a981a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.25966012e-01,  7.18332008e-02,  4.55194086e-01, ...,\n",
       "        -4.68805969e-01,  1.53829992e-01, -1.86475694e-01],\n",
       "       [-1.56653095e-02,  1.38920948e-01,  3.21190864e-01, ...,\n",
       "         2.95290053e-01,  4.26099747e-01,  1.07050180e-01],\n",
       "       [-1.37113512e-01,  1.15588516e-01,  5.44600129e-01, ...,\n",
       "        -3.55696261e-01,  3.08481008e-02, -8.81578475e-02],\n",
       "       ...,\n",
       "       [-2.65266806e-01, -4.55446832e-04,  4.52478230e-01, ...,\n",
       "        -5.27608156e-01,  4.08464447e-02, -3.14140409e-01],\n",
       "       [ 1.54658139e-01,  7.55799040e-02,  4.91281509e-01, ...,\n",
       "        -1.36029109e-01, -1.38581023e-01,  3.17474872e-01],\n",
       "       [ 8.42434093e-02, -2.92996764e-02,  6.34216249e-01, ...,\n",
       "        -2.15925038e-01,  2.71637589e-02, -1.14496119e-01]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e76339-ca6e-4ae5-9bcc-1909c1be5a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4448e0-a006-495a-8728-0882290a12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.classifier = nn.Linear(input_dim, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        logits = self.classifier(input_ids)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Define input dimension \n",
    "input_dim = X_train_tf.shape[1]\n",
    "num_labels = 2\n",
    "\n",
    "# Initialize the custom model\n",
    "model = SimpleClassifier(input_dim=input_dim, num_labels=num_labels)\n",
    "\n",
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.embeddings[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "\n",
    "train_dataset = EmbeddingsDataset(X_train_tf, y_train_tf)\n",
    "eval_dataset = EmbeddingsDataset(X_val_tf, y_val_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6103dd61-440e-4995-81eb-cfddd8be1793",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4410' max='4410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4410/4410 00:54, Epoch 30/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.319200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.315300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4410, training_loss=0.3187799674313085, metrics={'train_runtime': 54.0221, 'train_samples_per_second': 2602.27, 'train_steps_per_second': 81.633, 'total_flos': 0.0, 'train_loss': 0.3187799674313085, 'epoch': 30.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=30,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.25,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eef6fcbf-9286-4e70-838a-8332a598a194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8727893053872232\n"
     ]
    }
   ],
   "source": [
    "# Get predictions on the validation dataset\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "f1 = f1_score(y_val_tf, predicted_labels, average='weighted')  # or 'macro', 'micro' depending on your needs\n",
    "\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf04a24-76ec-4308-94c7-1a3e100fbbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1: 0.8727893053872232"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9518b7f5-aeda-40fe-a418-7aa613cc0c4a",
   "metadata": {},
   "source": [
    "### Random Forest with Transformer Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3001e012-18cb-4ea6-83bd-70169b45c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is in array form\n",
    "\n",
    "X_train_embeddings = np.array(X_train_tf)\n",
    "X_val_embeddings = np.array(X_val_tf)\n",
    "y_train = np.array(y_train_tf)\n",
    "y_val = np.array(y_val_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "998512c5-b253-43f8-992f-1f7f8effdae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2/36 [02:15<38:25, 67.80s/it]\n",
      "100%|██████████| 1/1 [03:08<00:00, 188.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.8910158555197909\n",
      "Best Hyperparameters: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "F1 score on validation set with best parameters for Random Forest: 0.8898847631241997\n",
      "Classification Report for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1135\n",
      "           1       0.78      0.83      0.81       427\n",
      "\n",
      "    accuracy                           0.89      1562\n",
      "   macro avg       0.86      0.87      0.86      1562\n",
      "weighted avg       0.89      0.89      0.89      1562\n",
      "\n",
      "Confusion Matrix for Random Forest:\n",
      "[[1034  101]\n",
      " [  71  356]]\n",
      "F1 score on validation set for Random Forest: 0.8910158555197909\n"
     ]
    }
   ],
   "source": [
    "# Values for grid search\n",
    "parameters_rf = {\n",
    "    'n_estimators': [200],     \n",
    "    'max_depth': [None],      \n",
    "    'min_samples_split': [2],      \n",
    "    'min_samples_leaf': [1]     \n",
    "}\n",
    "\n",
    "best_f1_rf = 0\n",
    "best_params_rf = None\n",
    "\n",
    "# Progression bar\n",
    "total_combinations_rf = len(parameters_rf['n_estimators']) * len(parameters_rf['max_depth']) * len(parameters_rf['min_samples_split']) * len(parameters_rf['min_samples_leaf'])\n",
    "pbar_rf = tqdm(total=total_combinations_rf)\n",
    "\n",
    "# Perform grid search\n",
    "for n_estimators, max_depth, min_samples_split, min_samples_leaf in product(parameters_rf['n_estimators'], parameters_rf['max_depth'], parameters_rf['min_samples_split'], parameters_rf['min_samples_leaf']):\n",
    "\n",
    "    pbar_rf.update(1)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, random_state=0)\n",
    "    rf.fit(X_train_embeddings, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    rf_pred = rf.predict(X_val_embeddings)\n",
    "    f1_rf = f1_score(y_val, rf_pred, average='weighted')\n",
    "    \n",
    "    # Check if it's the best score\n",
    "    if f1_rf > best_f1_rf:\n",
    "        best_f1_rf = f1_rf\n",
    "        best_params_rf = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "        best_rf = rf  # Save the best model\n",
    "\n",
    "pbar_rf.close()\n",
    "\n",
    "print(f\"Best F1 Score: {best_f1_rf}\")\n",
    "print(f\"Best Hyperparameters: {best_params_rf}\")\n",
    "\n",
    "# Evaluate the best model on validation data\n",
    "print(\"F1 score on validation set with best parameters for Random Forest:\", best_rf.score(X_val_embeddings, y_val))\n",
    "print(\"Classification Report for Random Forest:\")\n",
    "print(classification_report(y_val, rf_pred))\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(confusion_matrix(y_val, rf_pred))\n",
    "\n",
    "f1_rf = f1_score(y_val, rf_pred, average='weighted')\n",
    "print(\"F1 score on validation set for Random Forest:\", f1_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a371f9-5bf2-41e2-b061-1a7279523fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best F1: 0.8910158555197909 ; For: {'n_estimators': 200, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab47627-e474-4d32-a3c0-a9b129eb6c93",
   "metadata": {},
   "source": [
    "## Predictions on Test\n",
    "##### Using the model with the best score in validation -> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11136257-b254-485c-8bbc-7821479a45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the merged datasets are DataFrames\n",
    "if isinstance(X_train_merged, np.ndarray):\n",
    "    X_train_merged = pd.DataFrame(X_train_merged)\n",
    "if isinstance(X_val_merged, np.ndarray):\n",
    "    X_val_merged = pd.DataFrame(X_val_merged)\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = pd.Series(y_train)\n",
    "if isinstance(y_val, np.ndarray):\n",
    "    y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "738f5760-ec6c-400d-957c-71ddeeed6e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decided to train on the entire dataset for the predictions on test set\n",
    "X_train_combined = pd.concat([X_train_merged, X_val_merged])\n",
    "y_train_combined = pd.concat([y_train, y_val])\n",
    "\n",
    "# Use model with best score from validation\n",
    "best_rf.fit(X_train_combined, y_train_combined)\n",
    "test_predictions = best_rf.predict(X_test_merged)\n",
    "\n",
    "# Creating the output file\n",
    "predictions_df = pd.DataFrame({'id': range(1, len(test_predictions) + 1), 'predicted': test_predictions})\n",
    "predictions_df.to_csv('Predictions_04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ada96e-7eeb-4847-be78-a5bb472dc1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## End of the notebook #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef285d-bc85-4bf7-b7b7-4d491fcc9cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### End of the notebook ##############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
